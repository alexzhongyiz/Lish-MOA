{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = ['train_features', 'train_targets_scored', 'train_targets_nonscored', 'test_features', 'sample_submission']\n",
    "\n",
    "df = {}\n",
    "for name in df_names:\n",
    "    df[name] = pd.read_csv(f\"../input/lish-moa/{name}.csv\", index_col=0)\n",
    "    print(f\"{name}: {df[name].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df['train_features'], df['train_targets_scored'].add_prefix('scored_'), df['train_targets_nonscored'].add_prefix('nonscored_')], axis=1)\n",
    "df_full['count_scored'] = df_full.filter(regex='^scored_').sum(axis=1)\n",
    "df_full['count_nonscored'] = df_full.filter(regex='^nonscored_').sum(axis=1)\n",
    "df_full['count_total'] = df_full['count_scored'] + df_full['count_nonscored']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_target_cols = list(df_full.filter(regex='^(scored|nonscored)_').columns)\n",
    "print(\"length of total targets, scored and non scored:\", len(total_target_cols))\n",
    "df_target_combine = df_full[[\"cp_type\"]+total_target_cols].groupby(total_target_cols).size().reset_index(name='Combination_Count').sort_values('Combination_Count', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_combine['Combination_Label'] = df_target_combine.index\n",
    "df_target_combine = df_target_combine[['Combination_Label', 'Combination_Count'] + total_target_cols]\n",
    "df_target_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = df_target_combine[['Combination_Label']+total_target_cols].set_index('Combination_Label')\n",
    "transition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_combine = pd.merge(df_full, df_target_combine, on=total_target_cols, how='left')\n",
    "df_full_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_trt = df_full_combine[df_full_combine.cp_type !='ctl_vehicle']\n",
    "df_train = df_full_trt.drop(columns= total_target_cols+['count_scored',\n",
    "       'count_nonscored', 'count_total', \n",
    "       'Combination_Count'] ).reset_index(drop = True).drop('cp_type', axis = 1)\n",
    "\n",
    "#df_test = df['test_features'][df['test_features']['cp_type']!='ctl_vehicle'].reset_index(drop = True).drop('cp_type', axis = 1)\n",
    "df_test = df['test_features'].drop('cp_type', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_trt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_label_target = df_full_trt.Combination_Label.reset_index(drop = True)\n",
    "df_target_OHE = pd.get_dummies(combination_label_target, prefix='combo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_OHE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2020, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = df_train.copy()\n",
    "\n",
    "for f, (train_index, val_index)  in enumerate(skf.split(X= df_train, y=combination_label_target)):\n",
    "    folds.loc[val_index, 'kfold'] = int(f)\n",
    "    \n",
    "folds['kfold'] =folds['kfold'].astype(int)\n",
    "folds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "    #the __len__ and __getitem__ are for torch.utils.data.DataLoader to load batches into neural networks.    \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    # this is to tell the model that it is in train mode, thus use batch normalization and dropout.\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        targets = targets.long()\n",
    "        targets = targets[:,0]\n",
    "        outputs = model(inputs)\n",
    "#         print(f\"target type: {targets.dtype}, target shape: {targets.shape}\")\n",
    "#         print(f\"outputs type: {outputs.dtype}, outputs shape: {outputs.shape}\")\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    #model.eval() is to disable batch normalization and dropout.\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        targets = targets.long()\n",
    "        targets = targets[:,0]\n",
    "\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "        \n",
    "        #this saves memory and accelerate the running time as we don't need to keep memory for the gradients.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, int(hidden_size*0.5)))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(int(hidden_size*0.5))\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(int(hidden_size*0.5), num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(folds).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c!='Combination_Label']\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n",
    "target_cols = [c for c in df_target_OHE.columns]\n",
    "print(len(feature_cols), len(target_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=2020):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 5\n",
    "EARLY_STOPPING_STEPS = 20\n",
    "EARLY_STOP = False\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=df_target_OHE.shape[1]\n",
    "hidden_size=2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single_fold_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to modify the dataframe names\n",
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(df_test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "#     train_target = df_target_OHE.iloc[trn_idx,:]\n",
    "#     val_target = df_target_OHE.iloc[val_idx,:]\n",
    "    \n",
    "    #x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    #x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "\n",
    "#     x_train, y_train = train_df[feature_cols].values, train_target.values\n",
    "#     x_valid, y_valid =  valid_df[feature_cols].values,val_target.values\n",
    "\n",
    "    x_train, y_train = train_df[feature_cols].values, train_df['Combination_Label'].values\n",
    "    y_train = np.reshape(y_train,(len(y_train),1))\n",
    "    y_train = y_train.astype(int)\n",
    "    \n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df['Combination_Label'].values\n",
    "#     print(f\" X_train shape: {x_train.shape}, y_train shape: {y_train.shape}, y_tran value: {y_train}\")\n",
    "    y_valid = np.reshape(y_valid,(len(y_valid),1))\n",
    "    y_valid= y_valid.astype(int)\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    #optimizer = torch.optim.RMsprop(model.parameters(), lr=LEARNING_RATE, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-3, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "    \n",
    "#     loss_fn = nn.BCEWithLogitsLoss()\n",
    "     \n",
    "    # try cross entropy loss since this taks seems to be the multi classfication task now\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "    \n",
    "#     oof = np.zeros((len(df_train), df_target_OHE.shape[1]))\n",
    "    oof = np.zeros((len(df_train), num_targets))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "#         print(f\"valid predicts shape: {valid_preds.shape} \")\n",
    "#         sys.exit()\n",
    "        \n",
    "        y_pred_label = np.argmax(valid_preds,axis=1)\n",
    "        print(f\"validation label accuary: {np.sum(y_pred_label == np.squeeze(y_valid))/len(y_pred_label)}\")\n",
    "#         sys.exit()\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \"\"\"\"\n",
    "    as a first step we train the combination targets and\n",
    "    just put the combination back using transition matrix of size (696* 608)\n",
    "    \n",
    "    Second step would be to use transition targets\n",
    "    \"\"\"\n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    #predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "#     predictions = np.zeros((len(test_), df_target_OHE.shape[1]))\n",
    "    predictions = np.zeros((len(test_), num_targets))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "#     oof = np.zeros((len(df_train), df_target_OHE.shape[1]))\n",
    "    oof = np.zeros((len(df_train),num_targets))\n",
    "# #     predictions = np.zeros((len(df_test), df_target_OHE.shape[1] ))\n",
    "    predictions = np.zeros((len(df_test), num_targets ))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = [42,2020,2,27]\n",
    "SEED = [2]\n",
    "# oof = np.zeros((len(df_train),df_target_OHE.shape[1] ))\n",
    "oof = np.zeros((len(df_train),num_targets))\n",
    "\n",
    "# predictions = np.zeros((len(df_test), df_target_OHE.shape[1]))\n",
    "predictions = np.zeros((len(df_test), num_targets))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "print(oof)\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_results = np.argmax(oof,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "train_label_counter = Counter(train_predict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_orignal_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_orignal_labels = transition.iloc[train_predict_results,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train_label = train_predict_orignal_labels.reset_index()['Combination_Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true_train_label\n",
    "y_pred = train_predict_results\n",
    "\n",
    "print(np.sum(y_true==y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize probabilities\n",
    "for i in range(oof.shape[0]):\n",
    "    oof[i,:] = oof[i,:]/np.sum(oof[i,:])\n",
    "    \n",
    "oof_original_target = np.matmul(oof, transition.to_numpy() )\n",
    "prediction_original_target = np.matmul(predictions,transition.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate train log loss\n",
    "train_log_loss = 0\n",
    "num_of_scored_targets = df['train_targets_scored'].shape[1]\n",
    "\n",
    "trt_index = df_full_combine[df_full_combine.cp_type !='ctl_vehicle'].index\n",
    "for i in range(num_of_scored_targets):\n",
    "    y_true = df['train_targets_scored'].iloc[trt_index,i].values\n",
    "    y_pred = oof_original_target[:,i]\n",
    "    train_log_loss += log_loss(y_true, y_pred)/num_of_scored_targets\n",
    "train_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_original_target.shape\n",
    "scored_target_cols = df['sample_submission'].columns\n",
    "sub = df['sample_submission'].copy()\n",
    "sub[scored_target_cols] = prediction_original_target[:,:len(scored_target_cols)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['test_features'].reset_index(drop = True, inplace = True)\n",
    "test_ctrl_index = df['test_features'][df['test_features'].cp_type == 'ctl_vehicle'].index\n",
    "\n",
    "sub.iloc[test_ctrl_index,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_clip(df, floor, ceiling):\n",
    "    for c in range(df.shape[1]):\n",
    "        df.iloc[:,c] = np.where(df.iloc[:,c]< bar, bar, df.iloc[:,c])\n",
    "        df.iloc[:,c] = np.where(df.iloc[:,c] > ceiling, ceiling, df.iloc[:,c])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_clipped = prob_clip(sub, 0.00015, 0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_clipped.to_csv('submission.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
